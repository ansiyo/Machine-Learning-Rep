{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansiyo/Machine-Learning-Rep/blob/main/PROJECT_34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbk3tYOnHNWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e715c6-a50c-4c4c-d939-7b9c68a0455a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.  2. ]\n",
            " [4.  3. ]\n",
            " [7.  2.5]]\n"
          ]
        }
      ],
      "source": [
        "# SIMPLEIMPUTER\n",
        "\n",
        "# Imputing Missing Numerical Values\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sample data with missing values\n",
        "data = np.array([[1, 2], [np.nan, 3], [7, np.nan]])\n",
        "\n",
        "# Initialize SimpleImputer to fill missing values with the column mean\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "# Fit the imputer on data and transform the data\n",
        "transformed_data = imputer.fit_transform(data)\n",
        "\n",
        "print(transformed_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing Missing Categorical Values\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sample data with missing categorical values\n",
        "data = pd.DataFrame({\n",
        "    'color': ['red', 'green', np.nan, 'blue', 'green'],\n",
        "    'size': ['S', np.nan, 'M', 'L', 'L']\n",
        "})\n",
        "\n",
        "# Initialize SimpleImputer to fill missing values with the most frequent value\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Fit and transform\n",
        "transformed_data = imputer.fit_transform(data)\n",
        "\n",
        "print(transformed_data)"
      ],
      "metadata": {
        "id": "vuLrV2r2HQgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas                              # SLIDE EXAMPLE\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Create data with missing values\n",
        "data = {'one': pandas.Series([1, 2, 5], index=['a', 'b', 'e']),\n",
        "        'two': pandas.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}\n",
        "# We create a dictionary data with two series: one and two.\n",
        "# The series have missing values because the indexes don't completely overlap.\n",
        "# one has no values for index 'c' and 'd', while two has no value for index 'e'.\n",
        "\n",
        "# Step 2: Convert to DataFrame\n",
        "table = pandas.DataFrame(data)\n",
        "print(\"Before:\\n\", table)\n",
        "# This converts the dictionary into a Pandas DataFrame, resulting in a table where some values are NaN (missing).\n",
        "\n",
        "# Step 3: Use SimpleImputer to fill missing values with the column mean\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "imputer.fit(table)\n",
        "# The fit() method calculates the mean for each column based on the non-missing values.\n",
        "\n",
        "# Step 4: Transform the data and display the output\n",
        "imputed_data = imputer.transform(table.values)\n",
        "print(\"After:\\n\", pandas.DataFrame(imputed_data, columns=['one', 'two']))\n",
        "# The transform() method replaces the missing values with the previously calculated means.\n",
        "# The imputed_data is then converted back into a DataFrame for easier readability, with the same column names."
      ],
      "metadata": {
        "id": "LW2Loi5_HQiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HANDLING CATEGORIAL VALUES\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Customer_id': pd.Series([1, 2, 3, 4, 5]),\n",
        "    'Loan_type': pd.Series(['Home Loan', 'Personal Loan', 'Education Loan', 'Home Loan', 'Credit Loan']),\n",
        "    'Income': pd.Series(['30K', '25K', '15K', '40K', '35K'])\n",
        "}\n",
        "\n",
        "loan_info = pd.DataFrame(data)\n",
        "# This line converts the dictionary into a pandas DataFrame called loan_info. A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.\n",
        "# TABLE FORM\n",
        "# Customer_id\t           Loan_type\t           Income\n",
        "# 1\t                      Home Loan\t             30K\n",
        "# 2\t                    Personal Loan\t           25K\n",
        "# 3\t                    Education Loan\t         15K\n",
        "# 4\t                     Home Loan\t             40K\n",
        "# 5\t                    Credit Loan\t             35K\n",
        "print(loan_info)"
      ],
      "metadata": {
        "id": "w5Icz6uvHQlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GOT_DUMMIES\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Customer_id': pd.Series([1, 2, 3, 4, 5]),\n",
        "    'Loan_type': pd.Series(['Home Loan', 'Personal Loan', 'Education Loan', 'Home Loan', 'Credit Loan']),\n",
        "    'Income': pd.Series(['30K', '25K', '15K', '40K', '35K'])\n",
        "}\n",
        "\n",
        "loan_info = pd.DataFrame(data)\n",
        "loan_info = pd.get_dummies(loan_info,prefix_sep='_',drop_first=True)\n",
        "# This line is key. It converts the text in the Loan_type and Income columns into new columns with numbers. Here's what happens:\n",
        "# Loan_type: For each type of loan (like 'Home Loan', 'Personal Loan', etc.), a new column is created, and a 1 or 0 is used to indicate if the customer has that type of loan.\n",
        "# Income: The same thing happens for the income values (like '30K', '25K', etc.).\n",
        "# drop_first=True:\n",
        "# This means the first category after oredering it (like 'credit loan' and '15K') is dropped to avoid duplication, but you still have all the necessary information to figure out the dropped category.\n",
        "print(loan_info)"
      ],
      "metadata": {
        "id": "LBVtP5mlHQnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LABEL ENCODER()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Creating the data dictionary with customer information\n",
        "data = {\n",
        "    'Customer_id': pd.Series([1, 2, 3, 4, 5]),\n",
        "    'Loan_type': pd.Series(['Home Loan', 'Personal Loan', 'Education Loan', 'Home Loan', 'Credit Loan']),\n",
        "    'Income': pd.Series(['30K', '25K', '15K', '40K', '35K'])\n",
        "}\n",
        "\n",
        "# Creating the DataFrame from the data dictionary\n",
        "loan_info = pd.DataFrame(data)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Imports the LabelEncoder class from the sklearn.preprocessing module, which is part of the scikit-learn library.\n",
        "labelencoder = LabelEncoder()\n",
        "# Creates an instance of LabelEncoder called labelencoder. This object will be used to transform the categorical data (like Loan_type and Income) into numeric labels.\n",
        "loan_info_upd= loan_info.apply(labelencoder.fit_transform)\n",
        "# the fit_transform method of LabelEncoder is applied. It assigns a unique numeric code to each distinct value in that column.\n",
        "# For example, if the column has values like 'Home Loan', 'Personal Loan', and 'Credit Loan', LabelEncoder will assign them numeric codes like 0, 1, 2, etc\n",
        "print(loan_info_upd)"
      ],
      "metadata": {
        "id": "2KqmJf47HQrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Label Encoding Process:\n",
        "When you apply LabelEncoder to a column, it transforms each unique value into a numeric code, starting from 0. The encoding is done based on the lexicographical (alphabetical) order of the unique values in the column.\n",
        "\n",
        "Step-by-Step Explanation of Label Encoding:\n",
        "For Loan_type:\n",
        "The unique loan types in the column are:\n",
        "['Home Loan', 'Personal Loan', 'Education Loan', 'Credit Loan']\n",
        "LabelEncoder sorts these values alphabetically:\n",
        "['Credit Loan', 'Education Loan', 'Home Loan', 'Personal Loan']\n",
        "Then, it assigns numeric codes starting from 0:\n",
        "'Credit Loan' → 0\n",
        "'Education Loan' → 1\n",
        "'Home Loan' → 2\n",
        "'Personal Loan' → 3\n",
        "So, after applying the encoding, the column Loan_type will have these numeric values in place of the text labels.\n",
        "\n",
        "For Income:\n",
        "The unique income values in the column are:\n",
        "['30K', '25K', '15K', '40K', '35K']\n",
        "LabelEncoder sorts these values alphabetically:\n",
        "['15K', '25K', '30K', '35K', '40K']\n",
        "Then, it assigns numeric codes starting from 0:\n",
        "'15K' → 0\n",
        "'25K' → 1\n",
        "'30K' → 2\n",
        "'35K' → 3\n",
        "'40K' → 4\n",
        "So, the income values are encoded into their respective numeric codes based on alphabetical sorting."
      ],
      "metadata": {
        "id": "e2zxLq2kvxB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE HOT ENCODER()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Creating the data dictionary with customer information\n",
        "data = {\n",
        "    'Customer_id': pd.Series([1, 2, 3, 4, 5]),\n",
        "    'Loan_type': pd.Series(['Home Loan', 'Personal Loan', 'Education Loan', 'Home Loan', 'Credit Loan']),\n",
        "    'Income': pd.Series(['30K', '25K', '15K', '40K', '35K'])\n",
        "}\n",
        "\n",
        "# Creating the DataFrame from the data dictionary\n",
        "loan_info = pd.DataFrame(data)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#  Imports the OneHotEncoder from the sklearn.preprocessing module. OneHotEncoder is used to convert categorical variables into a binary matrix (one-hot encoding).\n",
        "onehotencoder = OneHotEncoder()\n",
        "# Creates an instance of OneHotEncoder called onehotencoder. This object will be used to convert categorical values into one-hot encoded values.\n",
        "x = onehotencoder.fit_transform(loan_info).toarray()\n",
        "print(x)"
      ],
      "metadata": {
        "id": "Tz0TN7WuHQtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The toarray() method is used to convert a sparse matrix into a dense numpy array.\n",
        "\n",
        "Sparse Matrix vs Dense Array:\n",
        "Sparse Matrix:\n",
        "\n",
        "A matrix where most of the elements are zero.\n",
        "It's stored in a memory-efficient way by only keeping track of the non-zero elements, saving memory.\n",
        "OneHotEncoder often returns a sparse matrix by default to save space, especially when there are many categories (which would result in many zeros).\n",
        "Dense Array:\n",
        "\n",
        "A matrix where all elements, including zeros, are stored explicitly.\n",
        "This takes more memory but is easier to work with for printing and some types of computations."
      ],
      "metadata": {
        "id": "XnPajv3j1OR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST TRAIN SPLIT\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# train_test_split from sklearn.model_selection is used to split the dataset into training and testing sets.\n",
        "df= pd.read_csv(\"cereal.csv\")\n",
        "df.head()\n",
        "# first five rows of the DataFrame.\n",
        "\n",
        "# Assuming you have a DataFrame named 'df'\n",
        "\n",
        "# Restructuring the DataFrame\n",
        "x = df[['calories', 'protein']].values\n",
        "# This line selects the calories and protein columns from the DataFrame df and stores their values in the variable x. These are the input features that the model will learn from.\n",
        "y = df['rating'].values\n",
        "# The rating column is the target variable (the value we want to predict), and its values are stored in y.\n",
        "# So, x is a 2D array containing the calories and protein data, while y is a 1D array containing the cereal ratings.\n",
        "\n",
        "# Splitting the data-set into training and testing into 80%-20%\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "# train_test_split(x, y, test_size=0.2): This function splits the data into training and testing sets.\n",
        "# x (input features) and y (target values) are split into two parts: training and testing.\n",
        "# test_size=0.2: This means 20% of the data will be set aside for testing, and 80% will be used for training.\n",
        "#x_train: The training set for input features (80% of the calories and protein data).\n",
        "# x_test: The testing set for input features (20% of the calories and protein data).\n",
        "# y_train: The training set for the target variable (rating), corresponding to the training features.\n",
        "# y_test: The testing set for the target variable (rating), corresponding to the testing features.\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print('Trained data-set:', x_train.shape)\n",
        "# 80 rows for training, 2 features (calories, protein)\n",
        "print('Test data-set:', x_test.shape)\n",
        "# 20 rows for testing, 2 features (calories, protein)."
      ],
      "metadata": {
        "id": "tVxvJN8GrSFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BMMvBOSHQ0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crx7hnE3HQ2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1hUZW1d6HQ4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NiwTj3guHQ8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ah0nJ9KqHQ-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZVA2KCowHRB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hoWlA07KHRD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KTQdSSqCHRHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oo9XbhdEHRJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqXHkgrIHRNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RE585xMZHRPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6hjKlGO3HRTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2gfSIoiHRVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIjTT-RJHRZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}