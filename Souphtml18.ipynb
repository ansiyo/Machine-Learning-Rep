{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeDAdx2CuaVVxfnge5KJfL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansiyo/Machine-Learning-Rep/blob/main/Souphtml18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIsVswpIng6E"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: Set the URL you want to scrape\n",
        "url = 'https://example.com'  # Replace with your target website\n",
        "\n",
        "# Step 2: Send an HTTP request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 3: Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Step 4: Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Step 5: Extract and print the page title\n",
        "    print(\"Page Title:\", soup.title.string)\n",
        "\n",
        "    # Step 6: Extract and print all hyperlinks\n",
        "    print(\"\\nAll links on the page:\")\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        text = link.get_text(strip=True)\n",
        "        print(f\"{text}: {href}\")\n",
        "\n",
        "    # Step 7: Extract and print all headings (h1, h2, h3, h4, h5, h6)\n",
        "    print(\"\\nAll headings on the page:\")\n",
        "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
        "        print(heading.get_text(strip=True))\n",
        "\n",
        "    # Step 8: Extract and print all paragraphs\n",
        "    print(\"\\nAll paragraphs on the page:\")\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        print(paragraph.get_text(strip=True))\n",
        "\n",
        "    # Step 9: Extract and print all image sources\n",
        "    print(\"\\nAll image sources on the page:\")\n",
        "    for img in soup.find_all('img'):\n",
        "        src = img.get('src')\n",
        "        alt = img.get('alt', 'No alt text')\n",
        "        print(f\"Alt: {alt}, Source: {src}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "meTW7gcWn8gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://GeoAI.com'"
      ],
      "metadata": {
        "id": "SCVyzZmwoFBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.get(url)"
      ],
      "metadata": {
        "id": "s9Ektzz3vR3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Step 4: Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n"
      ],
      "metadata": {
        "id": "zgND9eiQvZVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Extract and print all hyperlinks\n",
        "    print(\"\\nAll links on the page:\")\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        text = link.get_text(strip=True)\n",
        "        print(f\"{text}: {href}\")\n",
        "\n",
        "    # Step 7: Extract and print all headings (h1, h2, h3, h4, h5, h6)\n",
        "    print(\"\\nAll headings on the page:\")\n",
        "    for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
        "        print(heading.get_text(strip=True))\n",
        "\n",
        "    # Step 8: Extract and print all paragraphs\n",
        "    print(\"\\nAll paragraphs on the page:\")\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        print(paragraph.get_text(strip=True))\n",
        "\n",
        "    # Step 9: Extract and print all image sources\n",
        "    print(\"\\nAll image sources on the page:\")\n",
        "    for img in soup.find_all('img'):\n",
        "        src = img.get('src')\n",
        "        alt = img.get('alt', 'No alt text')\n",
        "        print(f\"Alt: {alt}, Source: {src}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "qzn7BxCjwmE9",
        "outputId": "c5202896-c8af-437e-8f03-2cb219d85bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-739784546.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-739784546.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(\"\\nAll links on the page:\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    }
  ]
}