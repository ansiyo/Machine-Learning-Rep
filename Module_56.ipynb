{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansiyo/Machine-Learning-Rep/blob/main/Module_56.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yq0KXa2r4DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb29854b-2bab-4c8c-dbc3-eeb629a07dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging with Decision Trees Accuracy: 1.00%\n"
          ]
        }
      ],
      "source": [
        "# BAGGING\n",
        "\n",
        "# Importing necessary libraries\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "# BaggingClassifier: This is an ensemble learning method that combines the predictions of multiple classifiers (usually the same type, like decision trees) trained on different subsets of the data. Each classifier is trained on a randomly sampled subset (with replacement) of the original dataset.\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base learners\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                                  n_estimators=10,  # Number of trees\n",
        "                                  random_state=42)\n",
        "# estimator=DecisionTreeClassifier: Multiple decision trees will be trained on different subsets of the data.\n",
        "# n_estimators=10: Specifies that 10 decision trees will be trained. Each tree is trained on a different bootstrapped.\n",
        "\n",
        "# Train the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging with Decision Trees Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# AdaBoost (Adaptive Boosting) is a technique that combines multiple weak learners (usually decision trees) to form a strong learner.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "# X: The feature matrix with 1000 rows and 20 columns, representing the input data.\n",
        "# y: The target array containing the class labels (0 or 1 )\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "# n_estimators=50: The number of weak learners (base models) that will be combined. The default weak learner in AdaBoost is a decision tree.\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# Accuracy= Number of correct predictions / Total number of predictions\n",
        "print(f\"AdaBoost Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "2OUDw0KofNN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f22eaac-1e93-4888-d8eb-f5272689ba37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Initialize the Gradient Boosting classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "# n_estimators=100: This specifies the number of decision trees (or estimators) to be trained sequentially. In this case, 100 trees will be built one after the other, with each tree correcting the errors of the previous ones.\n",
        "# learning_rate=0.1: The learning rate controls how much each new tree contributes to the overall model. A lower learning rate makes the model more conservative, requiring more trees to build a strong model, but it also reduces the risk of overfitting.\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "_8Hlto-dfNcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc8ae76-9756-4c47-9330-cca0876c9727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the learning rate do?\n",
        "Gradient Boosting works by sequentially adding decision trees to correct the errors made by the previous ones.\n",
        "\n",
        "The learning rate is a scaling factor that adjusts the contribution of each new tree to the overall model.\n",
        "\n",
        "If the learning rate is high (e.g., 0.5 or 1.0), each new tree makes a larger correction to the model, quickly adapting to the errors made by the previous trees.\n",
        "If the learning rate is low (e.g., 0.1 or 0.01), each new tree makes a smaller correction, requiring more trees to achieve a strong model but also making the training process more cautious and controlled.\n",
        "Why is the learning rate set to 0.1?\n",
        "In practice, a learning rate of 0.1 is a common choice because it strikes a good balance between performance and generalization:\n",
        "\n",
        "Balanced Updates: With a learning rate of 0.1, each tree contributes meaningfully to the overall model, but not so much that it risks overfitting.\n",
        "More Trees, Better Performance: Since a lower learning rate requires more trees to reach the same level of accuracy, it allows the model to be refined over a larger number of iterations (trees).\n",
        "Thus, setting the learning rate to 0.1 helps to improve the model's performance while minimizing the risk of overfitting, though it's important to fine-tune the learning rate along with the number of trees (n_estimators) based on the specific dataset."
      ],
      "metadata": {
        "id": "JVgHQTMjhvJ6"
      }
    }
  ]
}